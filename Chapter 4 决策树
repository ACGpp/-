算法原理
逻辑角度，ifelse语句的组合
几何角度，根据某种准则划分特征空间
目的：将样本约分越纯，使得模型得出的结果准确率越高
决策树->优化选择算法->信息增益->增益率（基尼指数）->剪枝处理->预剪枝（后剪枝）
4.1 基本流程
	决策树（decision tree）：基于树结构来进行决策
		决策：把样本分类，回答该样本是否为正例这一问题
		结构：一个根节点，若干内部节点和若干叶节点，叶节点对应决策结果，其它每个节点对应于一个属性测试；每个系欸但包含的样本集合根据属性测试的结果被划分到叶子节点中
		从根节点到每个叶节点的路径对应一个判定测试序列
	决策树学习：
		目的：产生一颗泛化能力强，即处理未见示例能力强的决策树
		策略：分而治之（divide-and-conquer）
		算法：
		           生成节点
			如果D中的样本属于同一类别C
					将node标记为C类叶节点；
					Return
			End if
			If A=空 OR D中样本在A上取值相同 then{
				将node标记为叶节点，其类别标记为D中样本数最多的类；
				Return
			End if}
				从A中选择最优划分属性a*
			For a*中的每一个值a*v do{
				为node生成一个分支；
				令类别标及为D中在a*上取值为a*v的样本子集；
				If Dv为空 then{
					将分支节点标记为叶节点，其类别标记为D中样本最多的类；
					Return}
				Else{
					以TreeGenerate（Dv，A、{a*}）为分支节点
				End if}
			End for}
			输出：以node为根节点的一棵决策树
		递归情况：1.当前节点包含的样本全属于同一类比，无需划分
				 2.当前属性集为空，或者所有样本在所有属性上取值相同，无法划分
					此时，我们将当前节点标记为叶节点，并将其类别设定为该节点所包含样本最多的类别，利用当前节点的后验分布
				3.当前节点包含的样本集合为空，不能划分
					此时，把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别，利用当前节点的先验分布
4.2划分选择
	信息熵：度量样本集合纯度的指标
	信息增益：给不同分支节点加权，计算出的用属性a对样本集D进行划分所得，信息增益越大，使用属性a来进行划分所得的纯度提升越大
	增益率（gain ratio）：能减小因数目过大而造成影响的式子
	基尼指数（gini index）：反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，所以，gini（d）越小，数据集D的纯度越高
4.3剪枝处理
	剪枝：去掉一些分支以对付过拟合
		分类：
			1.预剪枝：在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化能力的提升，则停止划分并将当前节点标记为叶节点
				优点：1.降低过拟合风险
					  2.减少决策树的训练时间开销和测试时间开销
			2.后剪枝：先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行观察，若将该节点对应的字数替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点
				优点：1.欠拟合风险小，泛化性能往往优于预剪枝决策树
				缺点：要自底向上对树中的所有非叶节点进行逐一考察，所以训练时间开销比未剪枝决策树和预剪枝决策树要高
4.4连续与缺失值
	4.4.1连续值处理
		方法：连续属性离散化技术，常用的有二分法，将连续属性从小到大排序，取区间[ai，ai+1]的中位点为候选划分点，选取最优的划分点进行样本集合划分，这里采用信息熵的知识进行求解得出信息增益，
			然后就是一个个数据往里套
	4.4.2缺失值处理
		方法：对缺失值加权，然后推广信息增益计算式使其能用来计算有缺失值的样本，然后数据套娃
4.5多变量决策树
	概念：试图建立一个合适的线性分类器，该分类器在图表上以多段斜线的形式呈现
	
summerize：
	决策树的基本思想就是如何最优化分类的问题，在本书的例子中，只有两类：好瓜和坏瓜，但是瓜有很多属性，这些属性决定了他们的好坏，于是，我们运用信息熵引出信息增益这一概念，来计算出怎样划分每一个分支，
	首先我们对不同属性的信息熵和信息增益进行计算 ，选取最大的（例色泽）作为划分属性，然后在该属性的不同值下再计算出不同属性的信息增益，得出最大的进行划分（例中为纹理=清晰下的根蒂（为最大的三者之一）），
	对其它属性依次进行计算即可得出决策树
	但，我们发现对于可取值数目较多的属性，信息增益会有所偏好，我们因此引入增益率来遏止其产生过重影响，但又因此引入了对可取值数目较少的属性有所偏好这一bug，所以我们先从候选划分属性中找出信息增益明显高于平
	均的属性，再从中选择增益率最高的
	除开信息熵，我们还可用基尼指数来划分属性
	得出了决策树后，我们并不直接投入使用，就像修饰花园的园丁们一样，我们要对其进行修剪，这样可以得到更好的结果也能预防过拟合和欠拟合，我们使用预剪枝和后剪枝两种方法，他们的原理差不多，区别在于一个是生成
	决策树后的方案，一个则相反，一个是自顶向下，后者是自底向上，核心思想是看看剪完该分支后对决策树产生的影响，致使结果优化的，执行；致使结果更差的，保留分支。
	接着我们提出两种再现实中常见的情况，一个是我们的数据是连续的，一个是我们的数据缺失了，解决方法分别是对连续的数据进行离散化，后者则是对缺失的属性进行加权，使其影响降低，具体的数学推导请参考南瓜书和西瓜书
	算法原理
 	再说一下感悟吧，概念性的东西如果自信记忆力够好没必要像我这样敲下来，我本人学习比较喜欢从头到尾梳理一遍，但是这样效率会很低，时间花的很多，一般一个十几分钟的视频，我停停写写笔记，可能会花多个10分钟
	，但如果比较抽象的东西，我可能写一遍之后最多再读两遍就能理解，这是好处，但是我注意力不太集中的时候会让我觉得很浪费时间，我今天就有点烦，我是怎么想到这么低效率的学习方式的也不明白，抄一遍可能记忆更深
	刻一点点吧，我觉得看视频如果一直看，一堂课下来实际你能记住的就是个大概，细节什么的就是缺胳膊少腿的，所以总结很重要，这一节的总结做下来之后，我会发现一些我其实理解错的概念和我认为自己会其实不会的概念，
	实际就是挑战自己的脑力，总结可以更高强度地调动自己脑子加深对知识的印象，我做总结是看着书做的，更好的方式是学完后不看书总结，然后梳理不足和错误点，这些方式其实大家都知道，只是没采取行动，我这几篇笔记做
	下来，也算是获得了一些对总结的正向反馈吧，不知道能不能支持我跳出舒适圈
	再总结一下感悟
		1.看完书，看完视频后及时总结，不仅是细节上的填充，更是要对框架的梳理
		2.不看书梳理，第一遍过框架+部分细节，第二遍看书补细节
		3.看书梳理，脑海里大概有个印象，然后填细节知识进去，对框架一步步调试，最后做个总结，像不像机器学习？哈哈哈哈哈哈
		4.这么做的目的是增强记忆，加强理解，查漏补缺
	讲一下为什么保留感悟的长篇大论，我想展现出我的思考过程出来吧，其实一开始感悟没有总结，但是写着写着会发现自己的感悟里有几个核心的要素，我们把它提取出来，就发现我们究竟感悟出个什么东西，然后也能知道它是怎么来的，
	怎么来的很重要，人们常举一些例子证明观点，感悟的过程就是这论证的过程，只不过太杂太乱，我们需要对式子进行整理，所以其实是感悟->搭建框架->提取要点->总结
