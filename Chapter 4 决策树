算法原理
逻辑角度，ifelse语句的组合
几何角度，根据某种准则划分特征空间
目的：将样本约分越纯，使得模型得出的结果准确率越高
决策树->优化选择算法->信息增益->增益率（基尼指数）->剪枝处理->预剪枝（后剪枝）
4.1 基本流程
	决策树（decision tree）：基于树结构来进行决策
		决策：把样本分类，回答该样本是否为正例这一问题
		结构：一个根节点，若干内部节点和若干叶节点，叶节点对应决策结果，其它每个节点对应于一个属性测试；每个系欸但包含的样本集合根据属性测试的结果被划分到叶子节点中
		从根节点到每个叶节点的路径对应一个判定测试序列
	决策树学习：
		目的：产生一颗泛化能力强，即处理未见示例能力强的决策树
		策略：分而治之（divide-and-conquer）
		算法：
		           生成节点
			如果D中的样本属于同一类别C
					将node标记为C类叶节点；
					Return
			End if
			If A=空 OR D中样本在A上取值相同 then{
				将node标记为叶节点，其类别标记为D中样本数最多的类；
				Return
			End if}
				从A中选择最优划分属性a*
			For a*中的每一个值a*v do{
				为node生成一个分支；
				令类别标及为D中在a*上取值为a*v的样本子集；
				If Dv为空 then{
					将分支节点标记为叶节点，其类别标记为D中样本最多的类；
					Return}
				Else{
					以TreeGenerate（Dv，A、{a*}）为分支节点
				End if}
			End for}
			输出：以node为根节点的一棵决策树
		递归情况：1.当前节点包含的样本全属于同一类比，无需划分
				 2.当前属性集为空，或者所有样本在所有属性上取值相同，无法划分
					此时，我们将当前节点标记为叶节点，并将其类别设定为该节点所包含样本最多的类别，利用当前节点的后验分布
				3.当前节点包含的样本集合为空，不能划分
					此时，把当前节点标记为叶节点，但将其类别设定为其父节点所含样本最多的类别，利用当前节点的先验分布
4.2划分选择
	信息熵：度量样本集合纯度的指标
	信息增益：给不同分支节点加权，计算出的用属性a对样本集D进行划分所得，信息增益越大，使用属性a来进行划分所得的纯度提升越大
	增益率（gain ratio）：能减小因数目过大而造成影响的式子
	基尼指数（gini index）：反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率，所以，gini（d）越小，数据集D的纯度越高
4.3剪枝处理
	剪枝：去掉一些分支以对付过拟合
		分类：
			1.预剪枝：在决策树生成过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化能力的提升，则停止划分并将当前节点标记为叶节点
				优点：1.降低过拟合风险
					  2.减少决策树的训练时间开销和测试时间开销
			2.后剪枝：先从训练集生成一颗完整的决策树，然后自底向上地对非叶节点进行观察，若将该节点对应的字数替换为叶节点能带来决策树泛化性能提升，则将该子树替换为叶节点
				优点：1.欠拟合风险小，泛化性能往往优于预剪枝决策树
				缺点：要自底向上对树中的所有非叶节点进行逐一考察，所以训练时间开销比未剪枝决策树和预剪枝决策树要高
4.4连续与缺失值
	4.4.1连续值处理
		方法：连续属性离散化技术，常用的有二分法，将连续属性从小到大排序，取区间[ai，ai+1]的中位点为候选划分点，选取最优的划分点进行样本集合划分，这里采用信息熵的知识进行求解得出信息增益，然后就是一个个数据往里套
	4.4.2缺失值处理
		方法：对缺失值加权，然后推广信息增益计算式使其能用来计算有缺失值的样本，然后数据套娃
4.5多变量决策树
	概念：试图建立一个合适的线性分类器，该分类器在图表上以多段斜线的形式呈现
	
summerize：
	决策树的基本思想就是如何最优化分类的问题，在本书的例子中，只有两类：好瓜和坏瓜，但是瓜有很多属性，这些属性决定了他们的好坏，于是，我们运用信息熵引出信息增益这一概念，来计算出怎样划分每一个分支，首先我们对不同属性的信息熵和信息增益进行计算 ，选取最大的（例色泽）作为划分属性，然后在该属性的不同值下再计算出不同属性的信息增益，得出最大的进行划分（例中为纹理=清晰下的根蒂（为最大的三者之一）），对其它属性依次进行计算即可得出决策树
	但，我们发现对于可取值数目较多的属性，信息增益会有所偏好，我们因此引入增益率来遏止其产生过重影响，但又因此引入了对可取值数目较少的属性有所偏好这一bug，所以我们先从候选划分属性中找出信息增益明显高于平均的属性，再从中选择增益率最高的
	除开信息熵，我们还可用基尼指数来划分属性
	得出了决策树后，我们并不直接投入使用，就像修饰花园的园丁们一样，我们要对其进行修剪，这样可以得到更好的结果也能预防过拟合和欠拟合，我们使用预剪枝和后剪枝两种方法，他们的原理差不多，区别在于一个是生成决策树后的方案，一个则相反，一个是自顶向下，后者是自底向上，核心思想是看看剪完该分支后对决策树产生的影响，致使结果优化的，执行；致使结果更差的，保留分支。
	接着我们提出两种再现实中常见的情况，一个是我们的数据是连续的，一个是我们的数据缺失了，解决方法分别是对连续的数据进行离散化，后者则是对缺失的属性进行加权，使其影响降低，具体的数学推导请参考南瓜书和西瓜书
