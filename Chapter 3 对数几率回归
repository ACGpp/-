对数几率回归（logistic regression）：用线性回归模型的预测结果去逼近真实标记的对数几率
	优点：1.直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确带来的问题
		2.可以得出近似概率预测，对许多需利用概率辅助决策的任务很有用
		3.对数几率函数是任意阶可导的凸函数，现有的许多数值优化算法都可以直接用于求取最优解
对于分类任务的回归学习：找一个单调可微分函数将分类任务的真实标记y与线性回归模型的预测值联系起来，我们能想到的最理想的是单位阶跃函数，但是由于其不连续，我们使用对数几率函数因为它在一定程度上近似单位阶跃函数且单调可微分
信息论：以概率论、随机过程为基本研究工具，研究广义通信系统的整个过程。常见应用有无损数据压缩（如zip文件）、有损数据压缩（如MP3和JPEG）
	自信息 ：
	            I(X)=-logb p(x),  p(x)为x的概率质量函数
		    当b=2时单位为bit，当b=e时单位为nat
	信息熵（自信息的期望）：度量随机变量的不确定性，信息熵越大越不确定
		    H(X)=E[I(X)]=- ∑p(x)logb p(x)     (此处以离散型为例）
		计算信息熵时约定：若p（x）=0，则p（x）logb p（x）=0       
	相对熵（KL散度）：度量两个分布的差异，其中典型使用场景是用来度量理想分布p（x）和模拟分布q（x）之间的差异 
无优化约束问题的求解方法             
	梯度下降法：
	1.在定义域中随机选取一个点x0，将其带入函数f（x）并判断此时f（x0）是否为最小值，如果不是，则找下一个点x1，且保证f（x1）<f（x0），然后接着判断f（x1）是否是最小值，如果不是则重复上述步骤继续迭代寻找x2，x3，。。。直到找到使得f（x）取到最小值的x*
	2.显然，必须在找到第t个点xt时，能进一步找到第t+1个点xt+1，且保证f（xt+1）<f（xt），
	3.梯度下降法利用梯度所指向的方向时函数值增大速度最快的方向这一特性，使得迭代时朝着梯度的反方向进行，进而实现函数值越迭代越小
	牛顿法：
	1.牛顿法对于选取第t+1个点xt+1时更严，要求通过泰勒公式在xt的领域内找到一个函数值比其更小的点，并且这个点必须是xt的邻域内的极小值点
对数几率回归算法的机器学习三要素
1.模型：线性模型，输出值的范围为【0，1】，近似阶跃的单调可微函数
2.策略：极大似然估计，信息论
3.算法：梯度下降，牛顿法
summerize：
	对数几率回归对于分类问题有很好的解决优势，对于分类问题，我们需要将分类任务的真实标记y和线性回归模型的预测值联系起来，我们首先考虑单位阶跃函数，但因为其不可微分性，我们需要找到一个与其近似的单调可微函数，对数几率函数就是这样一个函数，找到这个函数之后，我们要用它来实现我们的目的，即找到最优解，首先我们需要使用极大似然估计（或信息论）来找出w和b的估计函数即概率密度函数，其次我们要求出似然函数得到损失函数，然后我们通过牛顿法或者梯度下降法找出最优解，注意在使用极大似然估计前，我们用后验概率估计来重写对数几率函数
	待解决的疑问：1.牛顿法和梯度下降法的数学推导
				
