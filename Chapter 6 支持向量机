算法原理：从几何角度，对于线性可分数据集合，支持向量机就是找距离正负样本都最远的超平面，相比于感知机，其解是唯一的，且不偏不倚，泛化性能更好
策略：给定线性可分数据集X，设X中几何间隔最小的样本为（xmin，ymin），那么支持向量机找超平面的过程可转化为带约束条件的优化问题
支持向量（support vector）：样本空间中任意点x到超平面（w，b）的距离
间隔（margin）：两个异类支持向量到超平面的距离之和
几何间隔：加上yi，yi∈{1，-1}，若分类正确，当wx+b为正数时，yi也为正数，当wx+b为负数时，yi也为负数，而分母恒为正数；当分类不正确时，yi的值正常，wx+b的正负倒过来了，依次可分辨出是否正确分类
	对于数据集关于超平面的几何间隔：数据集X中所有样本点的几何间隔最小值
支持向量机
	模型：给定线性可分数据集X，支持向量机模型希望求得数据集X关于超平面的几何间隔gama达到最大的那个超平面，然后套上一个sign函数实现分类功能
	凸优化问题
		对一般的优化约束问题，若目标函数是凸函数（无论正负），约束集合是凸集，我们可以对其进行最大凸优化求解

软间隔与正则化
	在现实任务中，很难确定合适的核函数使得训练样本在特征空间中线性可分，所以，我们要允许支持向量机在一些样本上出错，为此，我们引入了”软间隔“
		硬间隔：所有样本均满足约束，即所有样本都必须划分正确
		软间隔：允许某些样本不满足约束（尽可能少）
		可以将必须严格执行的约束函数转化为具有一定灵活性的损失，合格的损失函数要求如下：
			1.当满足约束条件时，损失为0
			2.当不满足约束条件时，损失不为0，
			3.（可选）当不满足约束条件时，损失函数与其违反约束条件的程度成正比
			
支持向量回归（SVR）
	采用一个以f（x）=wTx+b为中心，宽度为2∈的间隔带，来拟合训练样本
	落在带子上的样本不计算损失，不在带子上的则以偏离带子的距离作为损失，然后以最小化损失的方式迫使间隔带从样本最密集的地方穿过，进而达到拟合训练样本的目的

summerize：
	向量机就是通过寻找超平面来分类的模式，在二分类问题中，超平面是一维的，我们可以用线性方程来描述超平面，接着我们要确保超平面准确地分割了正负样本，于是我们引进了间隔，当两个最接近超平面的样本的间隔最大时，我们找到了最适合的超平面，这有点难理解，想像在湖的两岸有很多房子，在两岸分别找到一个离湖最近的房子，把他们称为a，b，得出a和b分别距离湖的长度L1，L2，将L1和L2加起来，我们得到了最小几何间隔，因为没有别的分别处于两岸的房子他们的距离湖的直线距离相加起来会小于L1+L2.那么我们如何求得这两个房子呢，或者说，如何求出这个长度呢？因为我们得出的函数是凸函数，且我们在求它的最值，于是我们可以使用凸优化理论来解决它，但我们有更高效的方法，我们对得出的式子使用拉格朗日乘子法得到其对偶问题，后面的没怎么看懂写不出总结，请移步南瓜书和其教程
	anyway，我们发现现实生活中就是有写样本难以在特征空间中线性可分，于是我们引入软间隔这个概念，which is 我们在求出的线性模型上加一点小小的扩大，使它变成一个比较粗的线，那落在这条线上的点，比如正类，允许他们在负类的那一边，但一定在线上，我把它叫做模糊界限，那么我们如何得出这个线的粗细长度呢？很简单，在优化目标时，加上损失函数乘以常数C，损失函数用来判别，常数C用于帮助人们调控约束的软硬程度，即当C驱近无穷时，因为我们求的是最小值，所以我们必须使损失函数等于0来让优化函数最小，当然，这是最极端的情况，C的值可以自行定义，可是这里引入的损失函数不太好求解，我们使用替代损失函数来帮助更好的取得结果，他们的核心都是使损失函数变成0，1的选择题，他们有如下特性，当损失函数大于0时，替代损失函数的值等于多出来的那个值，如hinge函数中的1-损失函数值。这个值代表带子的宽度，然后求解
	而对于回归问题，支持向量机也有对应的支持向量回归方案，首先，他就像上面的软间隔问题一样，拥有一个比较粗的线，但不同的是，当样本不在这个线上时，我们称之为损失，即，我们不再是单纯用线来分割平面了，而是提供了一个类似棍子的长方体，它放在平面中，任何落到它里面的样本，我们称之为回归，在它之外的样本，我们称之为损失，那么我们为了确定这个长方体的位置，就要想办法最小化损失，即将尽可能多的样本囊括在长方体内，于是我们便可以对未知的样本进行预测分析（注意这里是针对回归问题，请重温分类与回归），所以，对于这里的优化函数，实则和上面的软间隔很像，区别是一个是敏感损失函数，一个是不敏感损失函数，具体推导请看南瓜书及其视频
	说点感悟，这个支持向量回归真的跟线性回归模型很像，让我感到费解的是，线性回归模型对于此类问题的解决效率似乎更好，而支持向量回归在我看来好像有点珠玉在前的感觉，对于向量机模型，我的感觉是，它太繁琐复杂，不太像是很正确的方向，可能它的用处还没有被开发出来，无论是对于分类问题还是回归问题，似乎都能被取代，或许是因为它跟线性回归模型和线性判别都太过类似，只是将线性模型变成了超平面概念，并且它不如线性模型，它甚至在这里只能是直线（依照我现在接触的知识来看），而线性模型可以是曲线，在现实生活中，分类问题用曲线来表达更加准确吧，非黑即白可不是什么好认知，当然，这只是我的浅显认知，毕竟我只是学习了它的一点皮毛，说不定在此领域有建树的人会有不同的高瞻，又或者，它的应用方向是错的。
