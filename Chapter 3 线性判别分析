Linear Discriminant Analysis：给定训练样例集，设法将样例投影到一条直线上，使同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别
算法原理：从几何的角度，让全体训练样本经过投影后：
		1.异类样本的中心尽可能远
		2.同类样本的方差尽可能小
损失函数推导:1.令正样本和负样本在x轴上的投影之间的距离最大，模长*μ*cosθ（μ为样本中心距离原点的直线距离）
		      2.令同类样本之间的距离最小，[w的转置*x（此处为x样本在x轴上的投影）-w的转置*μ（μ样本中心在x轴上的投影）]*的平方，对这个式子进行均值求取并求得其最小值
		二范数：求向量模长
 求解w：
		广义特征值：设A，B为n阶矩阵，若存在数λ，使得方程Ax=λBx存在非零解，则称λ为A相对于B的广义特征值，x为A相对于B的属于广义特征值λ的特征向量。特别的，当B=I（单位矩阵）时，广义特征值问题退化为标准特征值问题
		广义瑞利商：设A，B为n阶厄米矩阵，且B正定，称 （x不等于0）为A相对于B的广义瑞利商。特别的，当B=I（单位矩阵）时，广义瑞利商退化为瑞利商
最终我们将多分类问题拆解成多个二分类问题进行模拟
summerize：


summerize：
		这一段的数学推导真看的我云里雾里的，但好在视频里给出了详细的推理步骤，我这里给未来的自己填个坑，后面再来自己推导一遍，这几天最大的感悟就是数学太神奇了，数学家的脑洞也太科幻了，明明是那么简单的公式定理还有概念，却能把看起来无从下手的问题用简单的概念组合在一起得出一个式子再用相关的知识从各种角度解决这个问题。 
对于线性判别分析，其实就是分类问题 ，具体流程是先将其投影后的坐标表示出，接着我们为达成分类的目的，需要使同类样本的方差尽可能小，异类样本的中心尽可能远，为了做到这一点，我们
在几何上找到同类中心，异类中心和样本三者的关系，发现他们的数学函数式，接着我们定义类内散度矩阵和类间散度矩阵，用以简化推导出的函数式，接着我们发现w的大小实际上并不影响整体的值，所以，我们将分母进行固定（也可固定分子），然后我们用拉格朗日乘子法最小话上面的分子，约束为分母，
接着就是一个广义特征值问题，求得w的表达式，然后用广义瑞利商，这部分我没太搞懂，后面来填。

多分类学习
	拆解法：将多分类任务拆成若干个二分类任务求解
		首先，对问题进行拆分
		其次，为拆出的每个二分类任务训练一个分类器，在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果
		拆分策略：
		给定数据集D={（x1,y1),(x2,y2),…,(xm,ym)},yi∈{C1,C2,…,Cn}
			1.一对一（OvO）：将这N个类别两两配对，从而产生N(N-1)/2个二分类任务，例如OvO将为区分类别Ci和Cj训练一个分类器，该分类器把D中的Ci类样例作为正例，Cj类样例作为反例。在测试阶段，新样本将同时提交给所有分类器，于是我们将得到N（N-1）/2个分类结果
			2.一对其余（OvR）：每次将一个类的样例作为正例、所有其它类的样例作为反例来训练N个分类器。在测试时若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果，若多个分类器预测为正类，则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果
			3.OvO与OvR的比较：OvO训练的分类器更多，所以存储开销和测试时间开销更多，但在训练时OvR的每个分类器都使用全部训练样例，而OvO仅用到两个类的样例，多数情况下，他们的预测性能差不多
			训练的分类器个数 	存储开销	测试时间开销 	训练时间开销 	性能 	 策略
			N	更小	更小	类别多时更大	取决于具体的数据分布	OvR
			N（N-）/2	更大	更大	类别多时更小	取决于具体的数据分布	OvO    
			4.多对多（MvM）：每次将若干个类作为正类，若干个其他类作为反类，显然，OvO和OvR时MvM的特例。MvM的正反类构造必须有特殊的设计，不能随意选取，接下来介绍纠错输出码（Error Correcting Output Codes)
				ECOC:将编码的思想引入类别拆分，并尽可能在解码过程中具有容错性，工作步骤如下
					1.编码：对N个类别做M次划分，每次划分将一部分类别划为正类，一部分划为反类，从而形成一个二分类训练集；这样一共产生M个训练集，可训练出M个分类器
					2.解码：M个分类器分别对测试样本进行预测，这些预测标记组成一个编码。将这个预测编码与每个类别各自的编码进行比较，返回其中距离最小的类别作为最终预测结果
				类别划分：由编码矩阵指定
				编码矩阵：二元码：将每个类别分别指定为正类和反类
						三元码：在正反例之外，还可指定“停用类”
				ECOC的名字解释：因为在测试阶段，ECOC编码对分类器的错误有一定的容忍和修正能力。一般来说，对同一个学习任务，ECOC编码越长，纠错能力越强。然而，编码越长，意味着所需训练的分类器越多，计算、存储开销都会增大；另一方面，对有限类别数，可能的组合数目是有限的，码长超过一定范围后就失去了意义，对同等长度的编码，理论上，任意两个类别之间的编码距离越远，则纠错能力越强，所以，在码长较小时可更具这个原则计算出理论最优编码，但是，码长稍大一些就难以有效确定最优编码，但我们无需获得理论最优编码，一是因为非最优编码在实践中表现尚可，二是因为理论性质越好，并不代表分类性能越好
